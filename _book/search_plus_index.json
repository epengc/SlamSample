{"./":{"url":"./","title":"Introduction","keywords":"","body":"SlamPractice Here is a simple script to convert the equation into html: https://jsfiddle.net/8ndx694g/or use \\ Practice 1 simple cmake practice Create a Project named \"cmake_practice\" for a chance to writting CMakeList.txt file and getting a good orgnization of Project with ~/project/include and ~/project/src folds 6. Non Linear Optimization Problem 6.1 Status Estimation 6.1.1 Basic Status Estimation Equation Typical SLAM equation where is the camera's pose at the time , is the information of senser at the time . is the pixel coordinate perceived by camera at the time for landmark . Especaily, and can be any kind of functions. According to the camera's thoery, we have where, is the camera's internal parameters. is the distance between each two pixels. is the rotation matrix of the camera at the time , is the landmark. Furthermore, is the camera's translation at the time . When and are considered as Gaussain Noise in this scenario. For getting the minimum error for estimation of and , the method of least squares is employed to get the optimized solutions. With the knowledge of multidimensional Gaussain distribution For fomular , is , and is assumed to be . is assumed to be . As the same thoery, for formular , is , and is assumed to be . is assumed to be . 6.1.2 Least Square Cost Fucntion If we use the assumed value and prior assumption of multi-dimension Gaussain Distribution together, combining with the least square solution, the errors would be: And the cost function can be defined as: 6.2 Least Sqaure Optimization When consider a general least square problem: where, , is any non-linear function: We have four steps to get the numerical solution by iterations: Giving a initialized value For the iteration, seeking a small increament to have If is samller than the criterion, then stop. Else, let . 6.2.1 First Order and Second Order Gradient Solution If we have a Function and study the monotonicity within the range , by using Taylor Series, we have where, is the Jacobian matrix and is the Hessian matrix.If we only keep the first order gradient coefficient in Taylor serires, we have So when keep direction of and set to reduce the cost functon, we can get steepest descent method. But if we keep both first and second order component in Taylor series, the formula becomes The value of will lead cost function down to smallest value. Copyright © dsx2016.com 2019 all right reserved，powered by GitbookFile Modify: 2022-02-24 15:07:48 "}}